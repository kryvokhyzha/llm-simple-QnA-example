{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bffa3d799ee2485"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cc6c2bc923c1d3f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # linux\n",
    "# sudo apt-get install poppler-utils tesseract-ocr-all\n",
    "\n",
    "# # mac\n",
    "# brew install poppler\n",
    "# brew install tesseract --all-languages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e0c2ee4db074446",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d5c1befff013717",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import langchain\n",
    "import numpy as np\n",
    "import rootutils\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain.cache import RedisCache, RedisSemanticCache\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredPDFLoader, WikipediaLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.retrievers import ContextualCompressionRetriever, MultiVectorRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline, LLMChainExtractor\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.storage import RedisStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from redis import Redis\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from trulens_eval import Feedback, FeedbackMode, Select, Tru, TruChain\n",
    "from trulens_eval.app import App\n",
    "from trulens_eval.feedback import Groundedness, prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d8b8061024bd8de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider.openai import AzureOpenAI as fAzureOpenAI\n",
    "from trulens_eval.feedback.v2.feedback import ClassVar, Relevance, WithPrompt\n",
    "from unstructured.cleaners.core import clean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c650237af4a425d9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bf5eac230b5bdef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read local .env file\n",
    "_ = load_dotenv(find_dotenv())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda4f304c735be86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class debug_langchain:\n",
    "    def __init__(self, enable: bool = True):\n",
    "        self.enable = enable\n",
    "\n",
    "    def __enter__(self):\n",
    "        langchain.debug = True if self.enable else False\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        langchain.debug = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c52f554c7a8bdc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(max_attempt_number=10), wait=wait_exponential(multiplier=1, min=10, max=20))\n",
    "def call_tru_query_engine(recorder: TruChain, chain: BaseChatModel | BaseLLM, question: str) -> None:\n",
    "    with recorder:\n",
    "        _ = chain.invoke(question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a91b8403f6f1251",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SEPARATORS = [\"\\n\\n\", \"\\n\", r\"(?<=\\. )\", r\"(?<=\\! )\", r\"(?<=\\? )\", \" \", \"\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae56625d6b2ac0d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_root = rootutils.find_root(indicator=\".project-root\")\n",
    "path_to_data = path_to_root / \"data\" / \"my_documents\"\n",
    "path_to_db = path_to_root / \"data\" / \"db\"\n",
    "path_to_questions = path_to_root / \"data\" / \"questions\"\n",
    "\n",
    "path_to_db.mkdir(exist_ok=True, parents=True)\n",
    "path_to_data.mkdir(exist_ok=True, parents=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce0bb6bc2e81737"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_deployment_name = os.getenv(\"LLM_DEPLOYMENT_NAME\")\n",
    "embedding_deployment_name = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0445a14e5662b2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "redis_url = os.getenv(\"REDIS_URL\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ffb851cc52dc1bf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_questions_files_list = [\n",
    "    \"eval-questions.txt\",\n",
    "    # \"eval-surzhik-questions.txt\",\n",
    "    # \"eval-abstract-questions.txt\",\n",
    "]\n",
    "eval_questions = []\n",
    "\n",
    "for eval_question_file_name in eval_questions_files_list:\n",
    "    with open(path_to_questions / eval_question_file_name, \"r\") as file:\n",
    "        eval_questions.extend([line.strip() for line in file.readlines()])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b80d80969bf1c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "db_name = \"my-documents-db.sqlite\"\n",
    "tru = Tru(database_file=str(path_to_db / db_name))\n",
    "\n",
    "tru.run_dashboard(force=True)\n",
    "# tru.reset_database()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1565b64975bd43fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NoAnswerQuestionStatementRelevance(Relevance, WithPrompt):\n",
    "    prompt: ClassVar[PromptTemplate] = PromptTemplate.from_template(\n",
    "        \"\"\"You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.\n",
    "Respond only as a number from 0 to 10 where 0 is the least relevant and 10 is the most relevant.\n",
    "\n",
    "A few additional scoring guidelines:\n",
    "\n",
    "- Long RESPONSES should score equally well as short RESPONSES.\n",
    "\n",
    "- Answers that intentionally do not answer the question, such as 'I don't know' and model refusals, should be counted as the NOT RELEVANT and get score of 0.\n",
    "\n",
    "- Answers that do not answer the question, such as 'context does not provide information', should be counted as the NOT RELEVANT and get score of 0.\n",
    "\n",
    "- RESPONSE must be relevant to the entire PROMPT to get a score of 10.\n",
    "\n",
    "- RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.\n",
    "\n",
    "- RESPONSE that is RELEVANT to none of the PROMPT should get a score of 0.\n",
    "\n",
    "- RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.\n",
    "\n",
    "- RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.\n",
    "\n",
    "- RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.\n",
    "\n",
    "- RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.\n",
    "\n",
    "- RESPONSE that confidently FALSE should get a score of 0.\n",
    "\n",
    "- RESPONSE that is only seemingly RELEVANT should get a score of 0.\n",
    "\n",
    "- Never elaborate.\n",
    "\n",
    "PROMPT: {prompt}\n",
    "\n",
    "RESPONSE: {response}\n",
    "\n",
    "RELEVANCE: \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ExtendedAzureOpenAI(fAzureOpenAI):\n",
    "    def no_answer_relevance_with_cot_reasons(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response: str,\n",
    "    ) -> float | Tuple[float, Dict]:\n",
    "        \"\"\"Uses chat completion Model. A function that completes a template to\n",
    "        check the relevance of the response to a prompt. Also uses chain of\n",
    "        thought methodology and emits the reasons.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            prompt (str): A text prompt to an agent.\n",
    "            response (str): The agent's response to the prompt.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            float: A value between 0 and 1. 0 being \"not relevant\" and 1 being\n",
    "            \"relevant\".\n",
    "        \"\"\"\n",
    "        system_prompt = str.format(\n",
    "            NoAnswerQuestionStatementRelevance.prompt.template,\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "        )\n",
    "        system_prompt = system_prompt.replace(\"RELEVANCE:\", prompts.COT_REASONS_TEMPLATE)\n",
    "\n",
    "        return self._extract_score_and_reasons_from_response(system_prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dab3f6da21d49c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def build_feedbacks(rag_chain: BaseChatModel | BaseLLM) -> List[Feedback]:\n",
    "    feedbacks = []\n",
    "    azureopenai = ExtendedAzureOpenAI(deployment_name=llm_deployment_name)\n",
    "\n",
    "    # get context\n",
    "    if not isinstance(rag_chain.retriever, ContextualCompressionRetriever):\n",
    "        context = App.select_context(rag_chain)\n",
    "    else:\n",
    "        context = Select.Record.app.combine_documents_chain._call.args.inputs.input_documents[:].page_content\n",
    "\n",
    "    # Question/answer relevance between overall question and answer.\n",
    "    feedbacks.append(\n",
    "        Feedback(\n",
    "            azureopenai.relevance_with_cot_reasons,\n",
    "            name=\"Answer Relevance\",\n",
    "        ).on_input_output()\n",
    "    )\n",
    "\n",
    "    feedbacks.append(\n",
    "        Feedback(\n",
    "            azureopenai.no_answer_relevance_with_cot_reasons,\n",
    "            name=\"Answer Relevance (no answer)\",\n",
    "        ).on_input_output()\n",
    "    )\n",
    "\n",
    "    # Question/statement relevance between question and each context chunk.\n",
    "    if context:\n",
    "        feedbacks.append(\n",
    "            Feedback(azureopenai.relevance_with_cot_reasons, name=\"Context Relevance\")\n",
    "            .on_input()\n",
    "            # .on(context.collect())\n",
    "            .on(context)\n",
    "            .aggregate(np.max)\n",
    "        )\n",
    "\n",
    "    # is the response supported by the context\n",
    "    if context:\n",
    "        grounded = Groundedness(groundedness_provider=azureopenai)\n",
    "        feedbacks.append(\n",
    "            Feedback(\n",
    "                grounded.groundedness_measure_with_cot_reasons,\n",
    "                name=\"Groundedness\",\n",
    "            )\n",
    "            .on(context)\n",
    "            # .on(context.collect())\n",
    "            .on_output()\n",
    "            .aggregate(grounded.grounded_statements_aggregator)\n",
    "        )\n",
    "\n",
    "    return feedbacks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc7230fa32923c49",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare data\n",
    "\n",
    "+ unstructured PDFs: how to deal with tables, texts and images - [link](https://medium.com/@kbouziane.ai/harnessing-rag-for-text-tables-and-images-a-comprehensive-guide-ca4d2d420219)\n",
    "+ issue with PDFs loaders - [link](https://github.com/langchain-ai/langchain/issues/13805)\n",
    "+ langchain doc about unstructured PDFs (also includes postprocessing) - [link](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file)\n",
    "+ `AmazonTextractPDFParser` can be a greate option for PDFs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79eb5a21277d2e93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def describe_table(table: str) -> str:\n",
    "    describe_prompt_text = \"\"\"You are an assistant tasked with describing the table below. Provide detailed description of the information inside the table in human readable form. You can use bullet points, full sentences, or any other format you like. The description should be informative and very precise. Provide description in the language of the contents of the table. Table:\\n{element} \"\"\"\n",
    "    # Try to limit your description to 200 words\n",
    "    # Preferably, use the Ukrainian language for describing.\n",
    "\n",
    "    model = AzureChatOpenAI(\n",
    "        deployment_name=llm_deployment_name,\n",
    "        temperature=0,\n",
    "    )\n",
    "    describe_chain = (\n",
    "        {\"element\": lambda x: x} | ChatPromptTemplate.from_template(describe_prompt_text) | model | StrOutputParser()\n",
    "    )\n",
    "    described_tables = describe_chain.invoke(table)\n",
    "\n",
    "    return described_tables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3e14959d4394d4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _process_document_elements(\n",
    "    elements: List[Document],\n",
    ") -> List[Document]:\n",
    "    # TODO: generate additional documents with summary and hypothetical questions\n",
    "    new_elements = []\n",
    "    for element in elements:\n",
    "        if element.metadata[\"category\"] == \"Table\":\n",
    "            new_element = element.copy()\n",
    "            table_summary = describe_table(element.metadata[\"text_as_html\"])\n",
    "            new_element.page_content = f\"{table_summary}\"\n",
    "            new_elements.append(new_element)\n",
    "        else:\n",
    "            new_elements.append(element)\n",
    "    return new_elements\n",
    "\n",
    "\n",
    "def _combine_elements(\n",
    "    documents: List[Document],\n",
    "    additional_metadata: Optional[dict] = None,\n",
    ") -> Document:\n",
    "    additional_metadata = additional_metadata or {}\n",
    "    result = \"\\n\\n\".join([document.page_content for document in documents])\n",
    "    combined_metadata = {k: str(v) for k, v in documents[0].metadata.items()}\n",
    "    for document in documents[1:]:\n",
    "        for k, v in document.metadata.items():\n",
    "            v_str = str(v).strip()\n",
    "            if k in combined_metadata and combined_metadata[k] != v_str:\n",
    "                combined_metadata[k] += f\", {v_str}\"\n",
    "            elif k not in combined_metadata:\n",
    "                combined_metadata[k] = v_str\n",
    "\n",
    "    combined_metadata.pop(\"category\", None)\n",
    "    combined_metadata.pop(\"text_as_html\", None)\n",
    "    combined_metadata.pop(\"languages\", None)\n",
    "    combined_metadata.update(additional_metadata)\n",
    "    return Document(page_content=result, metadata=combined_metadata)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e6b8d19567c74ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_pdf_document(\n",
    "    file_path: str,\n",
    "    additional_metadata: Optional[dict] = None,\n",
    ") -> Document:\n",
    "    elements = UnstructuredPDFLoader(\n",
    "        file_path=file_path,\n",
    "        mode=\"elements\",\n",
    "        post_processors=[clean],\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=2800,\n",
    "        combine_text_under_n_chars=1000,\n",
    "        hi_res_model_name=\"yolox\",\n",
    "        languages=[\"eng\", \"ukr\", \"rus\"],\n",
    "    ).load()\n",
    "\n",
    "    return _combine_elements(\n",
    "        documents=_process_document_elements(elements=elements), additional_metadata=additional_metadata\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3058d45b124aa86",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_md_document(\n",
    "    file_path: str,\n",
    "    additional_metadata: Optional[dict] = None,\n",
    ") -> Document:\n",
    "    elements = UnstructuredMarkdownLoader(\n",
    "        file_path=file_path,\n",
    "        mode=\"elements\",\n",
    "        post_processors=[clean],\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=2800,\n",
    "        combine_text_under_n_chars=1000,\n",
    "        languages=[\"eng\", \"ukr\", \"rus\"],\n",
    "    ).load()\n",
    "\n",
    "    return _combine_elements(\n",
    "        documents=_process_document_elements(elements=elements), additional_metadata=additional_metadata\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a494a751203636e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loaders = [\n",
    "    WikipediaLoader(\n",
    "        query=\"Розпізнавання іменованих сутностей\", doc_content_chars_max=10000, load_max_docs=2, lang=\"uk\"\n",
    "    ),\n",
    "    WikipediaLoader(query=\"Дід Панас\", doc_content_chars_max=10000, load_max_docs=1, lang=\"uk\"),\n",
    "    WikipediaLoader(query=\"Grandpa Panas\", doc_content_chars_max=10000, load_max_docs=1, lang=\"en\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.append(_combine_elements(documents=loader.load(), additional_metadata={\"app_name\": \"main\"}))\n",
    "\n",
    "for file_name in os.listdir(path_to_data):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        docs.append(\n",
    "            load_pdf_document(file_path=str(path_to_data / file_name), additional_metadata={\"app_name\": \"main\"})\n",
    "        )\n",
    "\n",
    "for file_name in os.listdir(path_to_data):\n",
    "    if file_name.endswith(\".md\"):\n",
    "        docs.append(load_md_document(file_path=str(path_to_data / file_name), additional_metadata={\"app_name\": \"main\"}))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ec37dc171b3101c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=768,  # 768\n",
    "    chunk_overlap=128,  # 128\n",
    "    separators=SEPARATORS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1af7ab0adcda3d26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(splits)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "798120a93f38fb9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create embeddings and fill vector store"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "714844d158ee6ab0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(deployment=embedding_deployment_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c569ef0ddecdca0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = os.getenv(\"QDRANT_URL\")\n",
    "# collection_name = os.getenv(\"DOCUMENTS_COLLECTION_NAME\")\n",
    "collection_name = \"my_custom_documents\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63e69245799ec80a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    url=url,\n",
    "    collection_name=collection_name,\n",
    "    force_recreate=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15bb8dce36c3a653",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the document store and LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "105a5a13a6e083db"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "doc_store = Qdrant(\n",
    "    client=QdrantClient(url=url),\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embeddings,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57197bf24a3bbaaf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "temperature = 0.1\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=llm_deployment_name,\n",
    "    temperature=temperature,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2298e54c57bd568c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try out the search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "834814fe122ee51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"Що таке розпізнавання іменованих сутностей?\"\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "found_docs[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acdb7d180fc7c83c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"Хто такий дід Панас?\"\n",
    "found_docs = qdrant.max_marginal_relevance_search(query, k=2, fetch_k=10)\n",
    "found_docs[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d7ade496ca27f6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Redis cache"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "966157984b360632"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# cached_embedding = CacheBackedEmbeddings.from_bytes_store(\n",
    "#     embeddings, RedisStore(redis_url=redis_url), namespace=embeddings.model,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e742b5439e9a0c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set_llm_cache(\n",
    "#     RedisSemanticCache(redis_url=redis_url, embedding=embeddings, score_threshold=0.05)\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f78baba189cb109",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set_llm_cache(\n",
    "#     RedisCache(redis_=Redis.from_url(url=redis_url))\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f4ac0402589da88",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# llm1 = AzureChatOpenAI(\n",
    "#     deployment_name=llm_deployment_name,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b4728248b202d1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # The first time, it is not yet in cache, so it should take longer\n",
    "# llm1.invoke(\"Tell me a joke\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaa9b479a87734e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # The second time, while not a direct hit, the question is semantically similar to the original question,\n",
    "# # so it uses the cached result!\n",
    "# llm1.invoke(\"Tell me one joke\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c421bb249c2959e4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create simple RAG chain with map_rerank chain type"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bd8c8e6c6aa780a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# system_message = \"\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Answer the questions in the language of questions.\n",
    "#\n",
    "# In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
    "#\n",
    "# Question: [question here]\n",
    "# Helpful Answer: [answer here]\n",
    "# Score: [score between 0 and 100]\n",
    "#\n",
    "# How to determine the score:\n",
    "# - Higher is a better answer\n",
    "# - Better responds fully to the asked question, with sufficient level of detail\n",
    "# - If you do not know the answer based on the context, that should be a score of 0\n",
    "# - Don't be overconfident!\n",
    "#\n",
    "# Example #1\n",
    "#\n",
    "# Context:\n",
    "# ---------\n",
    "# Apples are red\n",
    "# ---------\n",
    "# Question: what color are apples?\n",
    "# Helpful Answer: red\n",
    "# Score: 100\n",
    "#\n",
    "# Example #2\n",
    "#\n",
    "# Context:\n",
    "# ---------\n",
    "# it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
    "# ---------\n",
    "# Question: якого типу була машина?\n",
    "# Helpful Answer: спортивний автомобіль або позашляховик\n",
    "# Score: 60\n",
    "#\n",
    "# Example #3\n",
    "#\n",
    "# Context:\n",
    "# ---------\n",
    "# Pears are either red or orange\n",
    "# ---------\n",
    "# Question: what color are apples?\n",
    "# Helpful Answer: This document does not answer the question\n",
    "# Score: 0\n",
    "#\n",
    "# Begin!\n",
    "#\n",
    "# Context:\n",
    "# ---------\n",
    "# {context}\n",
    "# ---------\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bc37af3b51ac537",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# system_message_prompt = PromptTemplate(\n",
    "#     template=system_message,\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     output_keys=['answer', 'score'],\n",
    "#     output_parser=RegexParser(regex='(.*?)\\\\nScore: (\\\\d*)', output_keys=['answer', 'score']),\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c343dc46efbfb0ec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain_type = \"map_rerank\"  # \"stuff\", \"map_reduce\", \"map_rerank\", and \"refine\".\n",
    "search_type = \"similarity\"\n",
    "k = 6\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        LongContextReorder(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=doc_store.as_retriever(\n",
    "        search_type=search_type, search_kwargs={\"k\": k, \"filter\": {\"app_name\": [\"main\"]}}\n",
    "    ),\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9baf66917c80243",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metadata = {\"temperature\": temperature, \"search_type\": search_type, \"chain_type\": chain_type}\n",
    "app_id = f'RAG {\", \".join([f\"{str(key)}: {str(value)}\" for key, value in metadata.items()])}'\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=build_feedbacks(rag_chain=qa_chain),\n",
    "    metadata=metadata,\n",
    "    feedback_mode=FeedbackMode.WITH_APP,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "275c2d8f42fa0d16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    call_tru_query_engine(recorder=tru_recorder, chain=qa_chain, question=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f63626a67fc65cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "records.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ea5bcbd3327c742",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create simple RAG chain with stuff chain type"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17d49681b5eea041"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"  # \"stuff\", \"map_reduce\", \"map_rerank\", and \"refine\".\n",
    "search_type = \"similarity\"\n",
    "k = 6\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        LongContextReorder(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=doc_store.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs={\"k\": k, \"filter\": {\"app_name\": [\"main\"]}},\n",
    "    ),\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b3464373f629f27",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# chain_type = \"stuff\"  # \"stuff\", \"map_reduce\", \"map_rerank\", and \"refine\".\n",
    "# search_type = \"mmr\"\n",
    "# k = 6\n",
    "# fetch_k = 12\n",
    "#\n",
    "# pipeline_compressor = DocumentCompressorPipeline(\n",
    "#     transformers=[\n",
    "#         LongContextReorder(),\n",
    "#     ]\n",
    "# )\n",
    "#\n",
    "# retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=pipeline_compressor,\n",
    "#     base_retriever=doc_store.as_retriever(search_type=search_type, search_kwargs={\"k\": k, 'fetch_k': fetch_k}),\n",
    "# )\n",
    "#\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=chain_type,\n",
    "#     retriever=retriever,\n",
    "#     return_source_documents=False,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cc589bf12e23d23"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metadata = {\"temperature\": temperature, \"search_type\": search_type, \"chain_type\": chain_type}\n",
    "app_id = f'RAG {\", \".join([f\"{str(key)}: {str(value)}\" for key, value in metadata.items()])}'\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=build_feedbacks(rag_chain=qa_chain),\n",
    "    metadata=metadata,\n",
    "    feedback_mode=FeedbackMode.WITH_APP,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec51e53c25643e63",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    call_tru_query_engine(recorder=tru_recorder, chain=qa_chain, question=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b5351f766543d26",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "records.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bed7429717e9ae8d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modify the chain to use custom prompt in English"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c2421713fc2a6cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "system_template = \"\"\"You are helpful assistant that use the following pieces of context to answer the user's question. If you don't know the answer and piece of answer can't be extracted from the context, just say that you don't know, don't try to make up an answer. Answer the questions in the language of questions. You can edit the context so that the answer looks more attractive to the user, but at the same time the essence and content do not change. It should look like a standalone answer on the question without mentioning the context and where the answer was found. Do not say you used the context to answer the question. You can ask clarifying questions if you need more information to answer the user's question, but try to avoid it.\n",
    "----------------\n",
    "{context}\"\"\"\n",
    "\n",
    "human_template = \"{question}\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44158a6fdb5ae091"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"  # \"stuff\", \"map_reduce\", \"map_rerank\", and \"refine\".\n",
    "search_type = \"similarity\"\n",
    "k = 6\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        LongContextReorder(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=doc_store.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs={\"k\": k, \"filter\": {\"app_name\": [\"main\"]}},\n",
    "    ),\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": ChatPromptTemplate.from_messages(messages)} if chain_type == \"stuff\" else None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ac22b6d57741d6e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metadata = {\"custom_prompt\": True, \"temperature\": temperature, \"search_type\": search_type, \"chain_type\": chain_type}\n",
    "app_id = f'RAG {\", \".join([f\"{str(key)}: {str(value)}\" for key, value in metadata.items()])}'\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=build_feedbacks(rag_chain=qa_chain),\n",
    "    metadata=metadata,\n",
    "    feedback_mode=FeedbackMode.WITH_APP,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc7f8e81efb5de00",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    call_tru_query_engine(recorder=tru_recorder, chain=qa_chain, question=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd04ea458b4fecf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "records.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42a4d3bad34161e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modify the chain to use custom prompt in English and context compression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed97764cd7f430ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_retriever_prompt = \"\"\"Given the following question and context, extract any part of the context AS IS that is relevant to answer the question. Remember, DO NOT edit the extracted parts of the context.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "custom_retriever_prompt_template = PromptTemplate(\n",
    "    template=custom_retriever_prompt, input_variables=[\"question\", \"context\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "313581bcedf72795"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"  # \"stuff\", \"map_reduce\", \"map_rerank\", and \"refine\".\n",
    "search_type = \"similarity\"\n",
    "k = 5\n",
    "\n",
    "base_retriever = doc_store.as_retriever(\n",
    "    search_type=search_type,\n",
    "    search_kwargs={\"k\": k, \"filter\": {\"app_name\": [\"main\"]}},\n",
    ")\n",
    "\n",
    "base_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[LongContextReorder(), LLMChainExtractor.from_llm(llm=llm, prompt=custom_retriever_prompt_template)]\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=base_compressor,\n",
    "    base_retriever=base_retriever,\n",
    "    k=k,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b95b70bd19173c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": ChatPromptTemplate.from_messages(messages)} if chain_type == \"stuff\" else None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1922ec873889dde5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metadata = {\"custom_prompt\": True, \"temperature\": temperature, \"search_type\": search_type, \"chain_type\": chain_type}\n",
    "app_id = f'RAG with compression {\", \".join([f\"{str(key)}: {str(value)}\" for key, value in metadata.items()])}'\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=build_feedbacks(rag_chain=qa_chain),\n",
    "    metadata=metadata,\n",
    "    feedback_mode=FeedbackMode.WITH_APP,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f508572855d3f9df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    call_tru_query_engine(recorder=tru_recorder, chain=qa_chain, question=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2ea43b1a146fac9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "records.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1e47391cccd494e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multivector Retrieval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3da8e1cfb9d8e9f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "id_key = \"doc_id\"\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "# collection_name = os.getenv(\"DOCUMENTS_COLLECTION_NAME\")\n",
    "collection_name = \"my_custom_documents_child\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a04b8d36f4b64c1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=url)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3601ed4ff534f661",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1536,\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "216808bbf6f15ce6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(deployment=embedding_deployment_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21dcfddf582d74cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "temperature = 0.0\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=llm_deployment_name,\n",
    "    temperature=temperature,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93d831162d1626a2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parent_store = RedisStore(\n",
    "    redis_url=redis_url,\n",
    "    namespace=\"parent-document-store\",\n",
    ")\n",
    "child_vectorstore = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embeddings,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d71c2d58006192b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "k = 6\n",
    "search_type = \"similarity\"\n",
    "\n",
    "multi_vector_retriever = MultiVectorRetriever(\n",
    "    vectorstore=child_vectorstore,\n",
    "    byte_store=parent_store,\n",
    "    id_key=id_key,\n",
    "    search_type=search_type,\n",
    "    search_kwargs={\"k\": k, \"filter\": {\"app_name\": [\"main\"]}},\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "865a88558da66412",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "doc_ids_map = {str(uuid.uuid4()): split for split in splits}\n",
    "doc_ids = list(doc_ids_map.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6323f697c1b0c90",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a child collection: chunking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8218d5f6f56a03"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=64,\n",
    "    separators=SEPARATORS,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baea93fd19cda114",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sub_docs = []\n",
    "for i, doc in enumerate(splits):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "        _doc.metadata[\"app_name\"] = doc_ids_map[doc_ids[i]].metadata[\"app_name\"]\n",
    "    sub_docs.extend(_sub_docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "184d9fb98d70bc70",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multi_vector_retriever.vectorstore.add_documents(sub_docs)\n",
    "multi_vector_retriever.docstore.mset(list(zip(doc_ids, splits)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65c28cadfe26a35d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create simple RAG chain with stuff chain type"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f3118b2f24428d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"  # \"stuff\", \"map_reduce\", \"map_rerank\", and \"refine\".\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        LongContextReorder(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=multi_vector_retriever,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": ChatPromptTemplate.from_messages(messages)} if chain_type == \"stuff\" else None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6255163d2f61f761",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metadata = {\"temperature\": temperature, \"search_type\": search_type, \"chain_type\": chain_type}\n",
    "app_id = (\n",
    "    \"RAG multivector chunking (256, 64) and custom prompt\"\n",
    "    f\" {', '.join([f'{str(key)}: {str(value)}' for key, value in metadata.items()])}\"\n",
    ")\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=build_feedbacks(rag_chain=qa_chain),\n",
    "    metadata=metadata,\n",
    "    feedback_mode=FeedbackMode.WITH_APP,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac50cdde58491cab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    call_tru_query_engine(recorder=tru_recorder, chain=qa_chain, question=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1db6c9a00622da6f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "records.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6662efe14ad07ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# t, _ =  tru.get_records_and_feedback(app_ids=[app_id])\n",
    "# t = t[['input', 'output', 'Answer Relevance', 'Answer Relevance (no answer)', 'Context Relevance', 'Groundedness']]\n",
    "# t['output'] = t['output'].apply(lambda x: x.encode().decode('unicode_escape'))\n",
    "# t['input'] = t['input'].apply(lambda x: x.encode().decode('unicode_escape'))\n",
    "# # t.to_excel(path_to_data.parent / 'test-results' / 'custom_promt_stuff_full.xlsx', index=False)\n",
    "# t.to_excel(path_to_data.parent / 'test-results' / 'multivector_chunking_256_64_full.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "355ac807a4da7b77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a child collection: summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f4003924b131aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | AzureChatOpenAI(deployment_name=llm_deployment_name)\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2098642850986881",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summaries = chain.batch(splits, {\"max_concurrency\": 5})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5055909293384ff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i], \"app_name\": doc_ids_map[doc_ids[i]].metadata[\"app_name\"]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20c2fa358c6b63eb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multi_vector_retriever.vectorstore.add_documents(summary_docs)\n",
    "multi_vector_retriever.docstore.mset(list(zip(doc_ids, splits)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f6464bae5066071",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create simple RAG chain with stuff chain type"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4db3659c06ad0ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain_type = \"stuff\"  # \"stuff\", \"map_reduce\", \"map_rerank\", and \"refine\".\n",
    "\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        LongContextReorder(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=multi_vector_retriever,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": ChatPromptTemplate.from_messages(messages)} if chain_type == \"stuff\" else None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123dc77697fa322f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metadata = {\"temperature\": temperature, \"search_type\": search_type, \"chain_type\": chain_type}\n",
    "app_id = (\n",
    "    \"RAG multivector summary and custom prompt\"\n",
    "    f\" {', '.join([f'{str(key)}: {str(value)}' for key, value in metadata.items()])}\"\n",
    ")\n",
    "\n",
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=build_feedbacks(rag_chain=qa_chain),\n",
    "    metadata=metadata,\n",
    "    feedback_mode=FeedbackMode.WITH_APP,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efd7b493b572e39b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for question in eval_questions:\n",
    "    call_tru_query_engine(recorder=tru_recorder, chain=qa_chain, question=question)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8168480d1efdc6d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "records.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "839369278310c583",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a child collection: hypothetical questions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21a1dc282d6aa56c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# functions = [\n",
    "#     {\n",
    "#         \"name\": \"hypothetical_questions\",\n",
    "#         \"description\": \"Generate hypothetical questions\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"questions\": {\n",
    "#                     \"type\": \"array\",\n",
    "#                     \"items\": {\"type\": \"string\"},\n",
    "#                 },\n",
    "#             },\n",
    "#             \"required\": [\"questions\"],\n",
    "#         },\n",
    "#     }\n",
    "# ]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f4b86d18706b3d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "# from langchain_openai import ChatOpenAI\n",
    "#\n",
    "# chain = (\n",
    "#     {\"doc\": lambda x: x.page_content}\n",
    "#     # Only asking for 3 hypothetical questions, but this could be adjusted\n",
    "#     | ChatPromptTemplate.from_template(\n",
    "#         \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n",
    "#     )\n",
    "#     | AzureChatOpenAI(\n",
    "#         deployment_name=llm_deployment_name,\n",
    "#     ).bind(\n",
    "#         functions=functions, function_call={\"name\": \"hypothetical_questions\"}\n",
    "#     )\n",
    "#     | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1469b8a31cb26074",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# chain.invoke(splits[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4031d947c2830553",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# hypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756dd26cf1b33c15",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# question_docs = []\n",
    "# for i, question_list in enumerate(hypothetical_questions):\n",
    "#     question_docs.extend(\n",
    "#         [Document(page_content=s, metadata={id_key: doc_ids[i], \"app_name\": doc_ids_map[doc_ids[i]].metadata['app_name']}) for s in question_list]\n",
    "#     )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c8cdcd3a81ca8e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# multi_vector_retriever.vectorstore.add_documents(question_docs)\n",
    "# multi_vector_retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7caa4ff9a7a3d51c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create simple RAG chain with stuff chain type"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26de2c2fde53de19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "49ea645344b23667"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
